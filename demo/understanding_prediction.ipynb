{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022 - for information on the respective copyright owner see the NOTICE file or the repository https://github.com/boschresearch/the-atlas-benchmark\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# System imports\n",
    "\n",
    "import yaml\n",
    "from context import Dataset\n",
    "from context import Predictor_CVM\n",
    "from context import Predictor_sof, Predictor_zan, Predictor_kara\n",
    "from context import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset object based on the yaml file configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested this instruction for the following datasets\n",
    "# input_dataset = './cfg/dataset_config_hotel.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_eth.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_thor1.yaml'\n",
    "input_dataset = '../cfg/dataset_config_thor3.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_atc.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_test_traj.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_test_point_obstacles.yaml'\n",
    "\n",
    "with open(input_dataset, 'r') as file:\n",
    "    benchmark_cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Optional split parameter defines the portion of the dataset that is being extracted,\n",
    "# frames between 0 * length(dataset) and 1 * length(dataset)\n",
    "dataset = Dataset(benchmark_cfg, split=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a list of scenarios with valid observations and complete ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_len = benchmark_cfg['benchmark']['setup']['observation period']\n",
    "prediction_horizon = benchmark_cfg['benchmark']['setup']['prediction horizon']\n",
    "\n",
    "# Here we iterate over all possible start frames in the dataset and extract the valid scenarios\n",
    "# with sufficient observations for all detected pedestrians and the ground truth data\n",
    "valid_scenes = dataset.extract_scenarios(prediction_horizon, observation_len, min_num_prople=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating a predictor and calculating predictions for one scene. This time we consider only one aprticle without uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../cfg/method_config_kara.yaml', 'r') as file:\n",
    "    method_cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "method_cfg['param']['uncertainty']['uncertainty'] = False\n",
    "predictor_certain = Predictor_kara(dataset, method_cfg, ['optimal','eth'])\n",
    "predictions = predictor_certain.predict(valid_scenes[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the predictions against the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "evaluation = Evaluator(set_GT_length=prediction_horizon)\n",
    "\n",
    "#evaluation.evaluate_scenario_ade_fde(valid_scenes[19], predictions)\n",
    "evaluation.plot_scenario(dataset, valid_scenes[100], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same visualization for an uncertain predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_cfg['param']['uncertainty']['uncertainty'] = True\n",
    "method_cfg['param']['uncertainty']['num_particles'] = 5\n",
    "method_cfg['param']['uncertainty']['uncertainty sigma'] = 0.3\n",
    "predictor_uncertain = Predictor_kara(dataset, method_cfg)\n",
    "predictions = predictor_uncertain.predict(valid_scenes[100])\n",
    "\n",
    "nll, _ = evaluation.cal_nll(valid_scenes[100],predictions)\n",
    "print(\"The NLL of the final position prediction is\", nll)\n",
    "evaluation.plot_scenario(dataset, valid_scenes[100], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making an animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "pathlib.Path('./gif').mkdir(parents=True, exist_ok=True)\n",
    "ani = evaluation.draw_animation(dataset, valid_scenes[100], predictions)\n",
    "ani.save('./gif/prediction_animation.gif', writer='imagemagick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "results_ade = []\n",
    "results_kade = []\n",
    "for i in range(len(valid_scenes)):\n",
    "    predictions = predictor_uncertain.predict(valid_scenes[i])\n",
    "    metric_values_ade = evaluation.evaluate_scenario_ade_fde(valid_scenes[i],predictions)\n",
    "    metric_values_kade = evaluation.evaluate_scenario_kade_kfde(valid_scenes[i],predictions)\n",
    "    results_ade.append(metric_values_ade)\n",
    "    results_kade.append(metric_values_kade)\n",
    "\n",
    "print('The mean ADE is', np.mean([result[0] for result in results_ade]), '+-', np.std([result[0] for result in results_ade]))\n",
    "print('The mean FDE is', np.mean([result[1] for result in results_ade]), '+-', np.std([result[1] for result in results_ade]))\n",
    "print('The mean kADE is', np.mean([result[0] for result in results_kade]), '+-', np.std([result[0] for result in results_kade]))\n",
    "print('The mean kFDE is', np.mean([result[1] for result in results_kade]), '+-', np.std([result[1] for result in results_kade]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is an exmaple on using the NLL metric with undertain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_cfg['param']['uncertainty']['uncertainty'] = True\n",
    "method_cfg['param']['uncertainty']['num_particles'] = 30\n",
    "\n",
    "method_cfg['param']['uncertainty']['uncertainty sigma'] = 0.1\n",
    "predictor_uncertain_low = Predictor_kara(dataset, method_cfg)\n",
    "\n",
    "method_cfg['param']['uncertainty']['uncertainty sigma'] = 0.5\n",
    "predictor_uncertain_avg = Predictor_kara(dataset, method_cfg)\n",
    "\n",
    "method_cfg['param']['uncertainty']['uncertainty sigma'] = 1.5\n",
    "predictor_uncertain_high = Predictor_kara(dataset, method_cfg)\n",
    "\n",
    "scene_id = 0\n",
    "\n",
    "nll_results = []\n",
    "for _ in range(10):\n",
    "    predictions = predictor_uncertain_low.predict(valid_scenes[scene_id])\n",
    "    nll, _ = evaluation.cal_nll(valid_scenes[scene_id], predictions)\n",
    "    nll_results.append(nll)\n",
    "print(\"The NLL of the final position prediction with a low uncertinty value is\", np.mean(nll))\n",
    "\n",
    "evaluation.plot_scenario(dataset, valid_scenes[scene_id], predictions)\n",
    "\n",
    "nll_results = []\n",
    "for _ in range(10):\n",
    "    predictions = predictor_uncertain_avg.predict(valid_scenes[scene_id])\n",
    "    nll, _ = evaluation.cal_nll(valid_scenes[scene_id], predictions)\n",
    "    nll_results.append(nll)\n",
    "print(\"The NLL of the final position prediction with an average uncertinty value is\", np.mean(nll))\n",
    "\n",
    "evaluation.plot_scenario(dataset, valid_scenes[scene_id], predictions)\n",
    "\n",
    "nll_results = []\n",
    "for _ in range(10):\n",
    "    predictions = predictor_uncertain_high.predict(valid_scenes[scene_id])\n",
    "    nll, _ = evaluation.cal_nll(valid_scenes[scene_id], predictions)\n",
    "    nll_results.append(nll)\n",
    "print(\"The NLL of the final position prediction with a high uncertinty value is\", np.mean(nll))\n",
    "\n",
    "evaluation.plot_scenario(dataset, valid_scenes[scene_id], predictions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25c18c03afd7d679242d2ee4b3c37ea4d94864fdb6c42e824a0531c5b7f80aa3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('atlas-env-release-test': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
