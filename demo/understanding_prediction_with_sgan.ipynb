{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022 - for information on the respective copyright owner see the NOTICE file or the repository https://github.com/boschresearch/the-atlas-benchmark\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# System imports\n",
    "\n",
    "import yaml\n",
    "from context import Dataset\n",
    "from context import Predictor_CVM\n",
    "from context import Predictor_sof, Predictor_zan, Predictor_kara, Newclass, TrajectronPredictor, SGANPredictor\n",
    "from context import Evaluator\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset object based on the yaml file configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested this instruction for the following datasets\n",
    "# input_dataset = '../cfg/dataset_config_hotel.yaml'\n",
    "input_dataset = '../cfg/dataset_config_eth.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_thor1.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_thor3.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_atc.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_test_traj.yaml'\n",
    "# input_dataset = '../cfg/dataset_config_test_point_obstacles.yaml'\n",
    "\n",
    "with open(input_dataset, 'r') as file:\n",
    "    benchmark_cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Optional split parameter defines the portion of the dataset that is being extracted,\n",
    "# frames between 0 * length(dataset) and 1 * length(dataset)\n",
    "dataset = Dataset(benchmark_cfg, split=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a list of scenarios with valid observations and complete ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation_len = benchmark_cfg['benchmark']['setup']['observation period']\n",
    "#prediction_horizon = benchmark_cfg['benchmark']['setup']['prediction horizon']\n",
    "\n",
    "observation_len = 8 # benchmark_cfg['benchmark']['setup']['observation period']\n",
    "prediction_horizon = 8 # benchmark_cfg['benchmark']['setup']['prediction horizon']\n",
    "\n",
    "# Here we iterate over all possible start frames in the dataset and extract the valid scenarios\n",
    "# with sufficient observations for all detected pedestrians and the ground truth data\n",
    "valid_scenes = dataset.extract_scenarios(prediction_horizon, observation_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating a predictor and calculating predictions for one scene. This time we consider only one aprticle without uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Verbose\n",
    "%tb\n",
    "with open('../cfg/method_config_trajectronpp.yaml', 'r') as file:\n",
    "    method_cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "method_cfg['uncertainty'] = False\n",
    "predictor_certain = SGANPredictor(valid_scenes[1], dataset)\n",
    "\n",
    "predictions = predictor_certain.predict(valid_scenes[1])\n",
    "\n",
    "print(f'Time spent for prediction {predictions.runtime}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the predictions against the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "evaluation = Evaluator(set_GT_length=prediction_horizon)\n",
    "\n",
    "# evaluation.evaluate_scenario_ade_fde(valid_scenes[19], predictions)\n",
    "evaluation.plot_scenario(dataset, valid_scenes[1], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same visualization for an uncertain predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_cfg['num_particles'] = 5\n",
    "predictor_uncertain = TrajectronPredictor(scenario=valid_scenes[15], dataset=dataset, method_cfg=method_cfg)\n",
    "predictions = predictor_uncertain.predict(valid_scenes[15])\n",
    "\n",
    "evaluation.plot_scenario(dataset, valid_scenes[15], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "results_ade = []\n",
    "results_kade = []\n",
    "for i in range(len(valid_scenes)):\n",
    "    predictions = predictor_uncertain.predict(valid_scenes[i])\n",
    "    metric_values_ade = evaluation.evaluate_scenario_ade_fde(valid_scenes[i],predictions)\n",
    "    metric_values_kade = evaluation.evaluate_scenario_kade_kfde(valid_scenes[i],predictions)\n",
    "    results_ade.append(metric_values_ade)\n",
    "    results_kade.append(metric_values_kade)\n",
    "\n",
    "print('The mean ADE is', np.mean([result[0] for result in results_ade]), '+-', np.std([result[0] for result in results_ade]))\n",
    "print('The mean FDE is', np.mean([result[1] for result in results_ade]), '+-', np.std([result[1] for result in results_ade]))\n",
    "print('The mean kADE is', np.mean([result[0] for result in results_kade]), '+-', np.std([result[0] for result in results_kade]))\n",
    "print('The mean kFDE is', np.mean([result[1] for result in results_kade]), '+-', np.std([result[1] for result in results_kade]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b4d84c3852f959c51452edc31f633b4d181e9890474db74d40ef74f9e3a5d4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('atlas-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
